{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5098191-d0cf-499a-abc3-d75ba2904870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install scikeras>=0.1.8\n",
    "# !pip install tensorflow>=2.3.0\n",
    "# !pip install -U skorch\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install pytorch-cpu #not sure if i need to fix this\n",
    "# !pip install s3fs\n",
    "# !pip install dask_kubernetes\n",
    "# !pip install pyarrow\n",
    "# !pip install xgboost\n",
    "!pip install dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef0c5f-d2ee-4faa-b74f-94a5c38e4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cloudpickle==2.1.0\n",
    "# !pip install dask==2022.05.0\n",
    "# !pip install distributed==2022.5.0\n",
    "# !pip install lz4==4.0.0\n",
    "# !pip install msgpack==1.0.3\n",
    "# !pip install toolz==0.11.2\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778a14f-241f-4857-ad70-ca4c4e65cc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5931b-fb8d-4203-a52c-0f6233280dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b8ce0-707e-49d7-aff4-2457356e5393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0679d6-6ee2-4ea6-bbad-69f7b6c39282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coiled.io/blog/tackling-unmanaged-memory-with-dask/\n",
    "# filename = 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2013-01.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cea44-bde4-4016-acd8-dee7b24260ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "# when working with clusters, specify cluster config, n_workers and worker_size\n",
    "client = Client(n_workers=4, \n",
    "                       threads_per_worker=1,\n",
    "                       memory_limit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235cfa24-0e6b-40ce-802e-17d76a0a222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474203d-6126-4e6f-a308-fb75346bbc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf4a53-abd2-41db-9393-f18673da0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-01.parquet'\n",
    "df = dd.read_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc0676-63c2-422a-b68f-c4a12e9420d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_load_nyc_taxi[]\n",
    "filename = './nyc_taxi/*.parquet'\n",
    "df_x = dd.read_parquet(\n",
    "    filename,\n",
    "    split_row_groups = 2\n",
    ")\n",
    "#end::ex_load_nyc_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5d8fe-ff51-4a48-b81a-54507dd9f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820a0f8-ab68-440c-867d-9acbe6cec430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a24d0-7e2f-444e-a15f-03cf4c208301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1825a1d-079e-43df-9d25-4d498e7d3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c84ae9-749a-43d4-afe4-fc1e274679ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcda7a6-b842-433f-9e3d-c306fa5c76c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9bddf-1864-4455-9ac1-2b3981f490d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_scaling_variables[]\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "df = dd.read_parquet(url)\n",
    "trip_dist_df = df[[\"trip_distance\", \"total_amount\"]]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(trip_dist_df)\n",
    "trip_dist_df_scaled = scaler.transform(trip_dist_df)\n",
    "trip_dist_df_scaled.head()\n",
    "\n",
    "#tag::ex_scaling_variables[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9712bf-8df7-4d2d-9883-c09e1ad7734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_categorical_variables[]\n",
    "from dask_ml.preprocessing import Categorizer\n",
    "from pandas.api.types import CategoricalDtype\n",
    "payment_type_amt_df = df[[\"payment_type\",\"total_amount\"]]\n",
    "\n",
    "cat = Categorizer(categories={\"payment_type\": CategoricalDtype([1,2,3,4])})\n",
    "categorized_df = cat.fit_transform(payment_type_amt_df)\n",
    "categorized_df.dtypes\n",
    "payment_type_amt_df.head()\n",
    "#tag::ex_categorical_variables[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1b402-8b04-46b9-9003-d44aea0e951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_dummy_variables[]\n",
    "from dask_ml.preprocessing import DummyEncoder\n",
    "\n",
    "dummy = DummyEncoder()\n",
    "dummified_df = dummy.fit_transform(categorized_df)\n",
    "dummified_df.dtypes\n",
    "dummified_df.head()\n",
    "#tag::ex_dummy_variables[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c07fe-61db-4d93-901e-93f6f618b098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99f99b-c8d8-4a9b-b781-aede8dffde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58607571-40fc-411e-941b-3b4bb877a3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63baabf3-255f-40f1-80af-7a50eb756a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457195b2-ce44-4812-b067-13916bbcf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ca48a-54ae-4800-99ac-a15307d2c772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5551e7-8f6f-4392-b095-98a3e9386fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_type_amt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b281844-42f1-49cb-865e-3bea8b461dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_type_amt_df.compute().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6ebf2-4943-4c65-a63c-45c0511e178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_locations_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077980b-2aa2-4586-9143-e2a88e091ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"PULocationID\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a8c88-fbbb-4247-b3fa-461e570a78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_joblib[]\n",
    "\n",
    "from dask.distributed import Client\n",
    "from joblib import parallel_backend\n",
    "\n",
    "client = Client('127.0.0.1:8786')\n",
    "\n",
    "X, y = load_my_data()\n",
    "net = get_that_net()\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    net,\n",
    "    param_grid={'lr': [0.01, 0.03]},\n",
    "    scoring='accuracy',\n",
    "    )\n",
    "\n",
    "XGBClassifier()\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    gs.fit(X, y)\n",
    "print(gs.cv_results_)\n",
    "\n",
    "#end::ex_joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60305a83-1a57-4a61-8d57-e01e35525ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d568e-9879-41da-954d-9a2986457cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_describe_percentiles[]\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "df.describe(percentiles = [.25, .5, .75]).compute()\n",
    "#end::ex_describe_percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f00326-f30f-4547-842d-e7617d6750ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_plot_distances[]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "f, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\n",
    "sns.despine(left=True)\n",
    "sns.distplot(np.log(df['trip_distance'].values+1), axlabel = 'Log(trip_distance)', label = 'log(trip_distance)', bins = 50, color=\"r\")\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#end::ex_plot_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d4fa6-4363-4840-aa23-7b5a7f8a48b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38443f01-fda0-4ef3-8f77-7355cabbdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that each col is a numpy ndarray. Note how array size is NaN until we call compute.\n",
    "# chunk sizes compte also shows how this is parallelized.\n",
    "df['trip_distance'].values.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc30d9f-4883-4b9c-8e64-4681d066cce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f6bc6-0dd5-47e1-ba2f-7cc446fcf529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows\n",
    "numrows = df.shape[0].compute()\n",
    "# number of columns\n",
    "numcols = df.shape[1]\n",
    "print(\"Number of rows {} number of columns {}\".format(numrows, numcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bc39d-ca93-4593-8924-b08ddd682f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).map(lambda x: x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2109960-a5ba-429c-aced-4c00c4e1ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_duration'].describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3061df-3306-46d8-9b31-8c4aff53b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_diff = np.abs(df['trip_duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3eb849-0257-4d94-b978-7e8992ef6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data as we see some dirty inputs\n",
    "df = df[df['trip_duration'] <= 10000]\n",
    "df = df[df['trip_duration'] >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e0707-35b3-45cc-bc8b-849ca764d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_duration'].describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11638ec-328d-4301-a15d-c9269e413131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note numpy -> ddf logic is slightly different. eg df[col].values vs df[col]\n",
    "# visualizing whole dataset is a different fish to fry, we are just showing small ones for now.\n",
    "plt.hist(df['trip_duration'], bins=100)\n",
    "plt.xlabel('trip_duration')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb0f71-cd72-4182-bfbc-7bbc8705eb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a63402-4594-4d79-9a69-414e89f0008c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['log_trip_duration'] = np.log(df['trip_duration'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c89f86-a5be-4f09-9d97-4e29b3a0427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['log_trip_duration'], bins=100)\n",
    "plt.xlabel('log(trip_duration)')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()\n",
    "sns.distplot(df[\"log_trip_duration\"], bins =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a390a-dcad-4887-bf4b-b401abe817ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a80305-055a-4517-b6dc-6c8bc2e8456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_dask_random_split[]\n",
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['trip_distance'], df['total_amount'])\n",
    "\n",
    "#end::ex_dask_random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553f6c6-2ccb-4620-a059-673e889c01f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2d84e-5ef4-4f60-b1bd-8f51a49b73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
    "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
    "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']],\n",
    "                                                    df[['total_amount']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507daa21-f5e6-4ee5-95da-1276a1a77639",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.categorize(\"VendorID\").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e1c2e-3a29-4eaf-a6a6-874d98b35a3e",
   "metadata": {},
   "source": [
    "\n",
    "Start the very tedious job of enriching the dataset, pulling features and categories out\n",
    "Chain them using dask, delay materialization...\n",
    "create dummy var out of labels.\n",
    "\n",
    "We could've read it at categorical when reading the parquet as specified in dtypes.\n",
    "Or we  can do it here.\n",
    "unlike pandas, must be categorized before calling dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67199a-4908-4853-8e82-95c84275f592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag::ex_categorical_variables_alt[]\n",
    "train = train.categorize(\"VendorID\")\n",
    "train = train.categorize(\"passenger_count\")\n",
    "train = train.categorize(\"store_and_fwd_flag\")\n",
    "\n",
    "test = test.categorize(\"VendorID\")\n",
    "test = test.categorize(\"passenger_count\")\n",
    "test = test.categorize(\"store_and_fwd_flag\")\n",
    "#tag::ex_categorical_variables_alt[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4b75a-e255-413c-95f6-cd469d865bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_datetime_dummy_alt[]\n",
    "train['Hour'] = train['tpep_pickup_datetime'].dt.hour\n",
    "test['Hour'] = test['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "train['dayofweek'] = train['tpep_pickup_datetime'].dt.dayofweek\n",
    "test['dayofweek'] = test['tpep_pickup_datetime'].dt.dayofweek\n",
    "\n",
    "train = train.categorize(\"dayofweek\")\n",
    "test = test.categorize(\"dayofweek\")\n",
    "\n",
    "dom_train = dd.get_dummies(train, columns = ['dayofweek'], prefix='dom', prefix_sep='_')\n",
    "dom_test = dd.get_dummies(test, columns=['dayofweek'], prefix='dom', prefix_sep='_')\n",
    "\n",
    "hour_train = dd.get_dummies(train, columns = ['dayofweek'], prefix='h', prefix_sep='_')\n",
    "hour_test = dd.get_dummies(test, columns=['dayofweek'], prefix='h', prefix_sep='_')\n",
    "\n",
    "dow_train = dd.get_dummies(train, columns = ['dayofweek'], prefix='dow', prefix_sep='_')\n",
    "dow_test = dd.get_dummies(test, columns=['dayofweek'], prefix='dow', prefix_sep='_')\n",
    "\n",
    "#tag::ex_datetime_dummy_alt[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08193e76-cc74-4104-afa6-14b1ac64a5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag::linear_regression[]\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "regr_df = df[['trip_distance', 'total_amount']].dropna()\n",
    "regr_X = regr_df[['trip_distance']]\n",
    "regr_y = regr_df[['total_amount']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    regr_X, regr_y)\n",
    "\n",
    "X_train = X_train.to_dask_array(lengths = [100]).compute()\n",
    "X_test = X_test.to_dask_array(lengths = [100]).compute()\n",
    "y_train = y_train.to_dask_array(lengths = [100]).compute()\n",
    "y_test = y_test.to_dask_array(lengths = [100]).compute()\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)\n",
    "\n",
    "#tag::linear_regression[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e98ef3-a89d-491c-97ae-021920891b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_daskml_cv[]\n",
    "import dask_ml.model_selection as dcv\n",
    "\n",
    "\n",
    "\n",
    "#tag::ex_daskml_cv[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d790a0-80e3-4735-b20f-86dcdf0ec1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_daskml_port[]\n",
    "from sklearn.linear_model import LinearRegression as ScikitLinearRegression\n",
    "from sklearn.linear_model import SGDRegressor as ScikitSGDRegressor\n",
    "estimators = [ScikitLinearRegression(), ScikitSGDRegressor()]\n",
    "run_tasks = [dask.delayed(estimator.fit)(X_train, y_train) for estimator in estimators]\n",
    "run_tasks\n",
    "#tag::ex_daskml_port[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692d191-6bae-4608-9e82-9e66fb7a2197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384d1ad-40cf-41f2-98a2-8b82a7dc2b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cba7b-914c-4940-84c0-73d31ad5dad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9667f05-c78d-4c54-8094-a8140cf0c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_train = dd.get_dummies(train, columns=[\"VendorID\"], prefix='vi', prefix_sep='_')\n",
    "test_train = dd.get_dummies(test, columns=[\"VendorID\"], prefix='vi', prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028f33f-d4d0-4b5c-8899-af10d04a8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_dask_dummy_alt[]\n",
    "vendor_train = dd.get_dummies(train, columns=[\"VendorID\"], prefix='vi', prefix_sep='_')\n",
    "vendor_test = dd.get_dummies(test, columns=[\"VendorID\"], prefix='vi', prefix_sep='_')\n",
    "\n",
    "passenger_count_train = dd.get_dummies(train, columns = ['passenger_count'], prefix='pc', prefix_sep='_')\n",
    "passenger_count_test =dd.get_dummies(test, columns= ['passenger_count'], prefix='pc', prefix_sep='_')\n",
    "store_and_fwd_flag_train = dd.get_dummies(train, columns = ['store_and_fwd_flag'], prefix='sf', prefix_sep='_')\n",
    "store_and_fwd_flag_test = dd.get_dummies(test, columns=['store_and_fwd_flag'], prefix='sf', prefix_sep='_')\n",
    "\n",
    "#tag::ex_dask_dummy_alt[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc1ded-26c0-478e-8385-a8e3729c776a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full list of categorical vars\n",
    "\n",
    "vendor_train = dd.get_dummies(train, columns=[\"VendorID\"], prefix='vi', prefix_sep='_')\n",
    "vendor_test = dd.get_dummies(test, columns=[\"VendorID\"], prefix='vi', prefix_sep='_')\n",
    "\n",
    "passenger_count_train = dd.get_dummies(train, columns = ['passenger_count'], prefix='pc', prefix_sep='_')\n",
    "passenger_count_test =dd.get_dummies(test, columns= ['passenger_count'], prefix='pc', prefix_sep='_')\n",
    "store_and_fwd_flag_train = dd.get_dummies(train, columns = ['store_and_fwd_flag'], prefix='sf', prefix_sep='_')\n",
    "store_and_fwd_flag_test = dd.get_dummies(test, columns=['store_and_fwd_flag'], prefix='sf', prefix_sep='_')\n",
    "\n",
    "# enrich the datetime into month/ hour / day, and turn it into dummy\n",
    "train['Month'] = train['tpep_pickup_datetime'].dt.month\n",
    "test['Month'] = test['tpep_pickup_datetime'].dt.month\n",
    "# harder way to to the same thing.\n",
    "# test['Month'] = (test['tpep_pickup_datetime']).map(lambda x: x.month)\n",
    "train['DayofMonth'] = train['tpep_pickup_datetime'].dt.day\n",
    "test['DayofMonth'] = test['tpep_pickup_datetime'].dt.day\n",
    "\n",
    "test.groupby('DayofMonth').count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00818a-964a-406d-8f74-cd5485cda2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391469a-944f-441f-9082-d5a25ad3b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Hour'] = train['tpep_pickup_datetime'].dt.hour\n",
    "test['Hour'] = test['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "train['dayofweek'] = train['tpep_pickup_datetime'].dt.dayofweek\n",
    "test['dayofweek'] = test['tpep_pickup_datetime'].dt.dayofweek\n",
    "\n",
    "train = train.categorize(\"Month\")\n",
    "test = test.categorize(\"Month\")\n",
    "\n",
    "train = train.categorize(\"DayofMonth\")\n",
    "test = test.categorize(\"DayofMonth\")\n",
    "\n",
    "train = train.categorize(\"dayofweek\")\n",
    "test = test.categorize(\"dayofweek\")\n",
    "\n",
    "month_train = dd.get_dummies(train, columns = ['dayofweek'], prefix='m', prefix_sep='_')\n",
    "month_test = dd.get_dummies(test, columns=['dayofweek'], prefix='m', prefix_sep='_')\n",
    "\n",
    "dom_train = dd.get_dummies(train, columns = ['dayofweek'], prefix='dom', prefix_sep='_')\n",
    "dom_test = dd.get_dummies(test, columns=['dayofweek'], prefix='dom', prefix_sep='_')\n",
    "\n",
    "hour_train = dd.get_dummies(train, columns = ['dayofweek'], prefix='h', prefix_sep='_')\n",
    "hour_test = dd.get_dummies(test, columns=['dayofweek'], prefix='h', prefix_sep='_')\n",
    "\n",
    "dow_train = dd.get_dummies(train, columns = ['dayofweek'], prefix='dow', prefix_sep='_')\n",
    "dow_test = dd.get_dummies(test, columns=['dayofweek'], prefix='dow', prefix_sep='_')\n",
    "# vendor_test = dd.get_dummies(test, columns=[\"VendorID\"], prefix='vi', prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c06fa4-d318-48b6-bb8d-7e6858d726f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and add average speed col\n",
    "train['avg_speed_h'] = 1000 * train['trip_distance'] / train['trip_duration']\n",
    "test['avg_speed_h'] = 1000 * test['trip_distance'] / test['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf8006-b501-4195-a886-acdb8b6a8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, sharey=True)\n",
    "ax[0].plot(train.groupby('Hour').avg_speed_h.mean().compute(), 'bo-', lw=2, alpha=0.7)\n",
    "ax[1].plot(train.groupby('dayofweek').avg_speed_h.mean().compute(), 'go-', lw=2, alpha=0.7)\n",
    "ax[2].plot(train.groupby('Month').avg_speed_h.mean().compute(), 'ro-', lw=2, alpha=0.7)\n",
    "ax[0].set_xlabel('Hour of Day')\n",
    "ax[1].set_xlabel('Day of Week')\n",
    "ax[2].set_xlabel('Month of Year')\n",
    "ax[0].set_ylabel('Average Speed')\n",
    "fig.suptitle('Average Traffic Speed by Date-part')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef67a4-bcd7-45d8-b920-862864eb5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = train.drop(['VendorID','passenger_count','store_and_fwd_flag', 'Month','DayofMonth','Hour','dayofweek'], axis = 1)\n",
    "test_final = test.drop(['VendorID','passenger_count','store_and_fwd_flag','Month','DayofMonth','Hour','dayofweek'], axis = 1)\n",
    "train_final = train_final.drop(['tpep_dropoff_datetime', 'tpep_pickup_datetime', 'trip_duration', 'avg_speed_h'], axis = 1)\n",
    "test_final = test_final.drop(['tpep_dropoff_datetime', 'tpep_pickup_datetime', 'trip_duration', 'avg_speed_h'], axis = 1)\n",
    "X_train = train_final.drop(['log_trip_duration'], axis=1)\n",
    "Y_train = train_final[\"log_trip_duration\"]\n",
    "X_test = test_final.drop(['log_trip_duration'], axis=1)\n",
    "Y_test = test_final[\"log_trip_duration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a927e98-3c29-442c-b226-4167fb203f24",
   "metadata": {},
   "source": [
    "#end enrich / category step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34cacd-6880-4465-8b86-9dd977fd5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_xgb_basic_usage[]\n",
    "import xgboost as xgb\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "n_workers = 4\n",
    "cluster = LocalCUDACluster(n_workers)\n",
    "client = Client(cluster)\n",
    "\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "\n",
    "booster = xgb.dask.train(\n",
    "    client,\n",
    "    {\"booster\": \"gbtree\",\"verbosity\": 2, \"nthread\": 4, \"eta\":0.01, gamma = 0,\n",
    "     \"max_depth\":5, \"tree_method\": \"auto\", \"objective\": \"reg:squarederror\"},\n",
    "    dtrain,\n",
    "    num_boost_round=4,\n",
    "    evals=[(dtrain, \"train\")])\n",
    "\n",
    "#end::ex_xgb_basic_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9762e1d-065d-4667-b337-08abf2a691f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like standard xgb Dmatrix, but note that we are explicitly passing in columns since we're dealing with Pandas, and that we need to give the colnames for xgb to know feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457e068-2d14-46e9-9106-2b1485564e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::ex_xgb_train_plot_importance[]\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns)\n",
    "dvalid = xgb.DMatrix(X_test, label=y_test,  feature_names=X_test.columns)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "xgb_pars = {'min_child_weight': 1, 'eta': 0.5, 'colsample_bytree': 0.9, \n",
    "            'max_depth': 6,\n",
    "'subsample': 0.9, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n",
    "'eval_metric': 'rmse', 'objective': 'reg:linear'}\n",
    "model = xgb.train(xgb_pars, dtrain, 10, watchlist, early_stopping_rounds=2,\n",
    "      maximize=False, verbose_eval=1)\n",
    "print('Modeling RMSLE %.5f' % model.best_score)\n",
    "\n",
    "xgb.plot_importance(model, max_num_features=28, height=0.7)\n",
    "\n",
    "pred = model.predict(dtest)\n",
    "pred = np.exp(pred) - 1\n",
    "#end::ex_xgb_train_plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e215f6-b792-4e60-ba11-42c1b775b32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag::ex_xgb_early_stopping_and_inference[]\n",
    "import xgboost as xgb\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "n_workers = 4\n",
    "cluster = LocalCUDACluster(n_workers)\n",
    "client = Client(cluster)\n",
    "\n",
    "def fit_model(client, X, y, X_valid, y_valid, early_stopping_rounds=5) -> xgb.Booster:\n",
    "    Xy_valid = dxgb.DaskDMatrix(client, X_valid, y_valid)\n",
    "    # train the model\n",
    "    booster = xgb.dask.train(\n",
    "    client,\n",
    "    {\"booster\": \"gbtree\",\"verbosity\": 2, \"nthread\": 4, \"eta\":0.01, gamma = 0,\n",
    "     \"max_depth\":5, \"tree_method\": \"gpu_hist\", \"objective\": \"reg:squarederror\"},\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    evals=[(dtrain, \"train\")])[\"booster\"]\n",
    "    return booster\n",
    "\n",
    "def predict(client, model, X):\n",
    "    predictions = xgb.predict(client, model, X)\n",
    "    assert isinstance(predictions, dd.Series)\n",
    "    return predictions\n",
    "\n",
    "#end::ex_xgb_early_stopping_and_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2830f-7acb-43e0-9431-7908aa4fe754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::dask_delayed_load_model[]\n",
    "@dask.delayed\n",
    "def load_model(path):\n",
    "    with fs.open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img\n",
    "#end::dask_delayed_load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82479b-455c-47d6-ad60-d2e201818cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067a4ea-9ef2-4101-a82d-699a888a37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::Dask_DataFrame_map_partition_inference[]\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "\n",
    "def rowwise_operation(row, arg*):\n",
    "    #row-wise compute\n",
    "    return result\n",
    "def partition_operation(df):\n",
    "    #partition wise logic\n",
    "    result = df[col1].apply(rowwise_operation)\n",
    "    return result\n",
    "\n",
    "ddf = dd.read_csv(“metadata_of_files”)\n",
    "results = ddf.map_partitions(partition_operation)\n",
    "results.compute()\n",
    "\n",
    "#An alternate way, but note the .apply() here becomes a pandas apply, not Dask .apply(), and you must define axis = 1\n",
    "ddf.map_partitions(lambda partition : partition.apply(lambda row: rowwise_operation(row), axis=1), meta=('ddf', object))\n",
    "\n",
    "#end::Dask_DataFrame_map_partition_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12deaacb-e1bf-47a4-bb28-71b9a4213f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f8416-c73f-4931-bdbd-0ff2a038a09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8bf7ab-0a22-4f93-8c47-4437ff9cf96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag::Dask_sql_define_tables[]\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.datasets\n",
    "from dask_sql import Context\n",
    "\n",
    "# read dataset\n",
    "taxi_df = dd.read_csv('./data/taxi_train_subset.csv')\n",
    "taxi_test = dd.read_csv('./data/taxi_test.csv')\n",
    "\n",
    "# create a context to register tables\n",
    "c = Context()\n",
    "c.create_table(\"taxi_test\", taxi_test)\n",
    "c.create_table(\"taxicab\", taxi_df)\n",
    "\n",
    "\n",
    "#end::Dask_sql_define_tables[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b076804-ed7a-48a8-81d0-6155b0c17b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464d8d5-4692-45fd-817c-97c5d8d31222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75ab74-9ce3-430d-8057-9fb5bdb2b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::Dask_sql_linear_regression[]\n",
    "\n",
    "# define model\n",
    "c.sql(\n",
    "\"\"\"\n",
    "CREATE MODEL fare_linreg_model WITH (\n",
    "    model_class = 'LinearRegression',\n",
    "    wrap_predict = True,\n",
    "    target_column = 'fare_amount'\n",
    ") AS (\n",
    "    SELECT passenger_count, fare_amount\n",
    "    FROM taxicab\n",
    "    LIMIT 1000\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# describe model\n",
    "c.sql(\n",
    "    \"\"\" \n",
    "DESCRIBE MODEL fare_linreg_model\n",
    "    \"\"\"\n",
    ").compute()\n",
    "\n",
    "# run inference\n",
    "c.sql(\n",
    "    \"\"\" \n",
    "SELECT\n",
    "    *\n",
    "FROM PREDICT(MODEL fare_linreg_model,\n",
    "    SELECT * FROM taxi_test\n",
    ")\n",
    "    \"\"\"\n",
    ").compute()\n",
    "\n",
    "#end::Dask_sql_linear_regression[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea24b8-3a68-443c-bdc4-6dd7efa5fd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87566ed6-dd6c-463b-941a-b349be52ba84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag::Dask_sql_XGBClassifier[]\n",
    "# define model\n",
    "c.sql(\n",
    "\"\"\"\n",
    "CREATE MODEL classify_faretype WITH (\n",
    "    model_class = 'XGBClassifier',\n",
    "    target_column = 'fare_type'\n",
    ") AS (\n",
    "    SELECT airport_surcharge, passenger_count, fare_type\n",
    "    FROM taxicab\n",
    "    LIMIT 1000\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# describe model\n",
    "c.sql(\n",
    "    \"\"\" \n",
    "DESCRIBE MODEL classify_faretype\n",
    "    \"\"\"\n",
    ").compute()\n",
    "\n",
    "# run inference\n",
    "c.sql(\n",
    "    \"\"\" \n",
    "SELECT\n",
    "    *\n",
    "FROM PREDICT(MODEL classify_faretype,\n",
    "    SELECT airport_surcharge, passenger_count, FROM taxi_test\n",
    ")\n",
    "    \"\"\"\n",
    ").compute()\n",
    "#end::Dask_sql_XGBClassifier[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083ad00-acef-4fcb-bba6-54169fa17294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37498d2f-d3da-43bc-a576-5f49196a2175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848553fc-1444-43f4-b196-7e2e4d403195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedaf84-8d73-45c8-aea0-acdbf330cd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abb8a4-9d3c-4faa-bccb-e7c563a4c2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490543fe-7eb9-4ff3-b355-289336980f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe69cad-3fab-4a8e-9b1d-210f437daf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075320f9-7f16-423b-acff-e61fed3bfae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419aa60-379c-453b-9937-4b16ef1acd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::batched_operations[]\n",
    "def handle_batch(batch, conn, nlp_model):\n",
    "    #run_inference_here.\n",
    "    conn.commit()\n",
    "\n",
    "def handle_partition(df):\n",
    "    worker = get_worker()\n",
    "    conn = connect_to_db()\n",
    "    try:\n",
    "        nlp_model = worker.roberta_model\n",
    "    except:\n",
    "        nlp_model = load_model()\n",
    "        worker.nlp_model = nlp_model\n",
    "    result, batch = [], []\n",
    "    for _, row in part.iterrows():\n",
    "        if len(batch) % batch_size == 0 and len(batch) > 0:\n",
    "            batch_results = handle_batch(batch, conn, nlp_model)\n",
    "            result.append(batch_results)\n",
    "            batch = []\n",
    "        batch.append((row.doc_id, row.sent_id, row.utterance))\n",
    "    if len(batch) > 0:\n",
    "        batch_results = handle_batch(batch, conn, nlp_model)\n",
    "        result.append(batch_results)\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "ddf = dd.read_csv(\"metadata.csv”)\n",
    "results = ddf.map_partitions(handle_partition)\n",
    "results.compute()\n",
    "#end::batched_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5cff8-65cd-4789-af83-002168d0e81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mika-dask]",
   "language": "python",
   "name": "conda-env-mika-dask-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
