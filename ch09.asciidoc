== Migrating Existing Analytic Engineering

.A Note for Early Release Readers
****
With Early Release ebooks, you get books in their earliest form&mdash;the authors' raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.

This will be the ninth chapter of the final book. The GitHub repo is available at https://github.com/scalingpythonml/scalingpythonml.

If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at vwilson@oreilly.com.
****

You likely have existing analytic workloads, and you may want to migrate some of them over to Dask as the scale of the data sizes you are working on increases.
This chapter will discuss the considerations, challenges, and experiences of users making the switch.
You will learn how to migrate existing big data workflows, including ETL (extract, transform and load) on Spark, as well as local workflows into Dask.

=== Why Dask

Here are some of the reasons to migrate into Dask.

* Python and PyData stack: the developers preferred using Python-native stack, and code style can remain consistently Pythonic.
* Infrastructure administration overhead: users who adopted Dask report less operations overhead.
* Reduced data copying: Pyspark users often need to turn Pandas dataframe to PySpark dataframe, to convert back to Pandas. Dask eliminates the intermediate step.
* Better ML integrations with Dask APIs: futures, delayed, and ML integrations require less glue code from the developer to maintain, as well as performance improvement from more flexible task graph management Dask offers.
* Fine-grained task management: Dask’s task graph is generated and maintained real-time during runtime, and users can access the task dictionary synchronously.
* Debugging overhead: Some developer teams prefer debugging experience in Python, as opposed to mixed python and java/scala stacktrace.
* Development overhead: The development step in Dask can be done locally with ease with the developer’s laptop, as opposed to needing to connect to a powerful cloud machine in order to experiment.
* Management UX: Dask visualization tools tend to be more visually pleasing and intuitive to reason, with native graphviz rendering for task graphs.

These are not all of the reasons, but if any of these speak to you, it's probably worth investing the time to consider moving the workload to Dask.
There are always tradeoffs involved, so the next section will look at some of the limitations followed by a roadmap to give you an idea of the scale of work involved in moving to Dask.

=== Limitations of Dask for ETL

Dask is relatively new, and the use of Python Data stack has historically focused on smaller scale problems.
At the time of writing, there are some limits to Dask for ETL.
Many of these limitations are being worked on by the community so check and see if they are still an issue.
Some of the scale limitations you should considered are as follows:

* Parquet scale limits: If parquet data exceeds 10TB scale, there are issues at the fastparquet and pyarrow level that slows Dask down, and metadata management overhead overwhelms.
* ETL workloads with Parquet files at 10TB scale and beyond that includes a mutation, such as append and update, runs into consistency issues.
* Weak datalake integrations: PyData stack has not engaged as much in the big data world traditionally, and the integrations on data lake management, such as Apache Iceberg, is missing
* High-level query optimization: Users of Spark would be familiar with the Catalyst optimizer that pushes down predicates for optimizing the physical work on the executors. This optimization layer is missing in Dask at the moment. Spark in its early years also did not have the Catalyst engine written yet, and there are work in progress to build this out for Dask.

For large scale pure ETL Dask may not be the right choice and leaving an existing system in place is a lot less work. However, if your building new functionality on top of existing workloads, the ability to use the Dask and PyData ecosystem can be worth the trade-offs.


=== Migration Roadmap

You should also consider the amount of work involved in moving existing workloads onto Dask.
For that, we’ve laid out an example of migration steps as a non-exhaustive list of items a team might want to think through when planning their move.


At the highest level, moving an existing workload to Dask involves both changing the code, as well as ensuring the correct environment is present.
Since it's easier to be able to develop against the environment you will be running, we suggest starting your migration with the environment.


==== Dask Deployment Environment

The deployment environment covers everything from the compute resources, to libraries, to access to the data your analytics depend on.
While you can move your code before you have a production environment setup, we think you should start with making sure you can get access to the compute and data you need.


===== Type of Clusters

Existing large scale ETL jobs will already have a cluster that they run on top of. Often these clusters are either cloud provisioned, or provided by an infrastructure group within your company.
Dask can work on the vast majority of cluster environments, and we cover how to deploy Dask on some common ones in <<ch14>>.
While it's tempting to pick the easiest to deploy environment it's often best to build on top of the ones which exist in your organization,
as they can simplify access data.

===== Libraries

It is important that your Dask workers all have the same libraries installed on them. Most clusters take care of this in various fashions, but in some -- like YARN -- Python dependency management is more challenging. You may need to work with your systems administrator / operations to install the libraries you need.
In our experience systems administrators / cluster operators are often very busy, so doing this early to give them time is beneficial.

[TIP]
====
If your organization has multiple clusters that are supported, choosing one where you can self-serve dependency management, like Kubernetes, is beneficial.
====


===== Data Sources

You likely have a shared filesystem already set up by your organization’s admin. Enterprise users are likely already using distributed data sources, running on HDFS, or blob storage like S3, which Dask works seamlessly.
Often there is some form of authentication required to access your organizations data, and this is also often managed by the cluster. If you have deployed your own cluster, you may need to set up special keys to be able to access your data.



In HPC (high performance computing) and other research environments, network-file-system (NFS) is more common. Since NFS "looks" like a regular filesystem to Dask you can read from it just as easily. NFS home directories (that is network drives which contain the files in your home dir) can cause issues for Dask, see more in <<ch14>>.


Dask can also read from less traditional file-like sources, including FTP and HTTP.  When working on an academic dataset that’s large and clunky to work with, like neuroimaging dataset that’s directly hosted by another organization, you may be able to directly read from their source filesystem. When using Dask this way, you should test out and consider network timeouts, load, and the expectations of the data provider. footnote:[While many academic institutions have fast connections, these can also be overwhelmed with very large datasets.]

[[ex_s3_minio_rw]]
.Read and Write to blob storage using Minio
====
[source, python]
----
include::./examples/dask/Dask-Ch10_porting.py[tags=ex_s3_minio_rw]
----
====

If your organization does not have a distributed filesystem in place, the link:$$https://landscape.cncf.io/card-mode?category=cloud-native-storage&grouping=category$$[CNCF has a non-exhaustive list of storage options].
In our experience, S3 compatible filesystems work well with Dask and many other tools in the data space.


As of this writing, Dask does not have integrated support for data lakes such as Iceberg or Deltalake. Instead you can use the Python libraries for link:$$https://iceberg.apache.org/docs/0.13.0/python-api-intro/$$[Iceberg] and link:$$https://mungingdata.com/dask/read-delta-lake/$$[Deltalake with Dask.].

==== Development: Considerations

Translating an existing logic to Dask is a fairly intuitive process once you've understood Dask's collections and execution model.
Depending on what your existing logic is written in -- e.g. Spark, or pandas -- you'll need to make different changes to your logic.
If your coming from a different programming language, like R, MATLAB, or Scala, you will need to rewrite all of your code even though logically the changes may be minimal.
We'll start with looking at migrating DataFrame based code since Dask also has DataFrames.


===== DataFrames

If you have a job that you are already running on a different platform, it’s likely you are already using columnar storage format, like parquet.
Unfortunately there is not a 1:1 mapping from parquet to Dask (or pandas) DataFrame types which can cause incorrect types. footnote:[For example a job which reads in and writes out a parquet file but makes no changes to the data, may result in a different parquet file being written.]
You should check the data types on read and write to catch potential correctness issues.
Rather than write the validation manually, pydantic can help you with validating the data types/schema of your data.
Once you inspect and determine type inference is imprecise, specifying dtypes can speed up your job a lot.
Additionally, it’s always a good idea to sanity-check strings, floating point numbers, datetime, and arrays.

[TIP]
====
Many systems -- e.g. Spark, Hive, Dask, etc. -- have their own parquet readers and writers which can result in some quirks. footnote:[A bit like Word and Corel back in the day]
If your having difficulty reading a dataset, finding out what tool created it and then searching for "pyarrow" + "toolname" type issue
can help you find out what special settings you might need.
====


When reading a large file from cloud storage into DataFrame, it is often useful to select columns ahead of time at dataframe read stage as covered in <<partitioned_reads>>.
This is especially important to take note of when porting code from platforms like Spark which have predicate pushdown, where even if you don’t quite specify columns desired, it would optimize and only read the required column for computation. Dask doesn’t quite provide that optimization yet.


Setting smart indices early in transformation of your DataFrame, prior to complex query, can speed things up. Be aware that multi-indexing is not supported by Dask yet. A common workaround for a multi-indexed DataFrame from other platform is mapping as a single concatenated column.


During transform stage, since Dask API gives users fine-grained control of how to distribute computation, it’s good to be mindful of when to introduce reduce operations on large data down to a single pandas dataframe. Methods such as repartition() and persist() are great way to introduce computations and reductions early on, before a shuffle operation such as a join().


[WARNING]
====
If you do a lot of mutation with DataFrames, migrating to Dask can involve a substantial rewrite.
====

You can not update the values in a Dask DataFrame as you can with a Pandas Dataframe. Since in-memory modification of a particular value is not possible, the only way to change a value is a map operation over the an whole column. If an in-memory value change is something you have to do often, that is better achieved through either the actor system or an external database.


===== Porting SQL to Dask

Dask does not natively offer SQL engine, and developers can either re-implement the logic in pandas / Dask API or try a SQL library on top of Dask.
The SQL engines built on top of Dask are focused on anlytic uses cases, if you need more than that (e.g. inserts, updates) consider using a separate database layer that is relational, such as Postgres, either before moving the data into Dask runtime, or after.

[[ex_postgres_dataframe]]
.Reading from a Postgres database
====
[source, python]
----
include::./examples/dask/Dask-Ch10_porting.py[tags=ex_postgres_dataframe]
----
====

Recently, fugue-sql project came on-line, which provides SQL compatibility to Pydata stack, including Dask. The project is in its infancy, but seems promising. Fugue can run its SQL queries using DaskExecutionEngine, or you can run fugue queries over Dask DataFrame you already are using. Alternatively, you can run a quick SQL query on dask dataframe on your notebook as well. Here’s an example of using fugue in notebook.

.Running SQL over dask dataframe with Fugue SQL
image::images/ch09/spwd0901.png[]

An alternate method is to use Dask-SQL library. This package uses Apache Calcite to provide the SQL parsing frontend, and is used to query Dask Dataframes. With that library, you can pass most of the SQL based operations to dask_sql context, and it will be handled. The engine handles standard SQL inputs like SELECT, CREATE TABLE, but also ML model creation, with CREATE MODEL syntax.

=== Conclusion

In this chapter you have reviewed the large questions and considerations you might put in when migrating existing analytic engineering work. You’ve also learned some of the similarities and dissimilarities of Dask, and what to look out for. Since Data Engineering on large scale tends to have similarities across many libraries, it’s often easy to overlook minute differences that leads to larger performance or correctness issues. Keeping them in mind will help you as you take your first journeys in Dask.
