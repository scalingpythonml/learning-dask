[[ch_3]]
== Understanding enough of how Dask works

.A Note for Early Release Readers
****
With Early Release ebooks, you get books in their earliest form&mdash;the authors' raw and unedited content as they writeâ€”so you can take advantage of these technologies long before the official release of these titles.

This will be the third chapter of the final book. Please note that the GitHub repo will be made active later on.

If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at vwilson@oreilly.com.
****

Now that you've run your first few tasks with Dask, it's time to learn a little bit about what's happening behind the scenes. Depending on if you are using Dask locally or in a distributed fashion the behavior can be a little different. While Dask does a good job of abstracting away many of the details of running on multiple threads or servers, having a solid grasp of how Dask is working will help you better decide both how and when to use it.

=== Execution Backends

Dask has many different execution backends, however, I find it easiest to think about them in two groups, local and distributed. With local backends, you are limited in scale to what a single computer can handle. Local backends also have advantages like avoiding network overhead, simpler library management, and dollar costfootnote:[Unless you work for a cloud provider and computers are close enough to free. If you do work for a cloud provider please send me cloud credits.].

==== Local Backends

Dask's three local backends are single-process, multithreaded, and multi-process. The single-process backend has no parallelism and is mostly useful for validating that a problem is caused by concurrency. Multi-threaded and multi-process backends are ideal for problems where the data is small, or the cost of copying it would be higher than computation time.

[TIP]
====
If you don't configure a specific local backend, Dask will pick the backend based on the library you are working with.
====

The local multithreaded scheduler is able to avoid having to serialize data and inter-process-communication costs. The multithreaded backend is suited for tasks where the majority of the computation is happening in native code, outside of Python. This is the case for many numeric libraries, such as pandas and numpy. If that is the case for you, you can configure Dask to use multithreading in the config:

.Dask Use Multithreading
====
[source, python]
----
include::./examples/dask/Dask-Ch3-Concepts.py[tags=threads]
----
====

The local multi-process backend has some additional overhead over multi-threaded, although it can be decreased on Unix & Unix-like systemsfootnote:[Including OSX & Linux]. The multi-process backend is able to avoid Python's global interpreter lock by launching separate processes. Launching a new process is more expensive than a new thread, and Dask needs to serialize data that moves between processes.footnote:[This also involves having to store a second copy of any object that is in the driver thread and then used in a worker. Since Dask shards it's collections this doesn't generally blow up as quickly as with "normal" multiprocessing.]

.Dask Use Multiprocess
====
[source, python]
----
include::./examples/dask/Dask-Ch3-Concepts.py[tags=process]
----
====

If you are running on a Unix system you can use the "forkserver" which will reduce the overhead of starting each Python interpreterfootnote:[https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods]. Using the forkserver will not reduce the communication overhead.

.Dask Use Multiprocess Forkserver
====
[source, python]
----
include::./examples/dask/Dask-Ch3-Concepts.py[tags=forkserver]
----
====

Dask's local backends are designed for performance rather than testing that your code will work on a distributed scheduler. To test that your code will run remotely, you should use Dask's Distributed scheduler with a "LocalCluster" instead.

==== Distributed (Dask Client)

Dask has one distributed scheduler backend, that talks to many different types of clusters, including a "LocalCluster". Each type of cluster is supported in its own library, and the DaskClient takes and will schedule a scheduler and workers.

The Dask Client is your entry point into the Dask distributed scheduler. In <<ex_dask_k8s>> will be using Dask with a Kubernetes cluster, and if you have another type of cluster or want details please see <<appa_clusterrs>>.

===== Libraries & Dependencies

Part of why Dask is so powerful is the Python ecosystem that it is in. While Dask will pickle and send our code to the workers, this doesn't include the libraries we use.footnote:[Automatically picking up and shipping the libraries would be very hard and also slow, although it can be done under certain circumstances.] To be able to take advantage of that ecosystem you need to be able to use additional libraries. During the exploration phase it is common to install packages at run-time as you discover that you need them.

The `PipInstall` worker plugin takes a list of packages and installs them at run-time on all of the workers. Looking back at the scraping example, to install bs4 you would call `distributed.diagnostics.plugin.PipInstall(["bs4"])`. Any new workers which are launched by Dask then need to wait for the package to be installed. The `PipInstall` plugin is ideal for quick prototyping when you are discovering which packages you need. You can think of `PipInstall` as the replacement for `!pip install` in a notebook over having a virtualenv.

To avoid the slowness of having to install packages each time a new worker is launched you should try and pre-install your libraries. Each cluster manager (e.g. YARN, Kubernetes, Coiled, Saturn, etc.) has its own ways for managing dependencies. This can be both run-time or setup-time where the packages are pre-installed. The specifics for the different cluster managers are covered in <<app_deploy>>.

With, for example, Kubernetes, the default startup script checks for the presence of some key environment variables (`EXTRA_APT_PACKAGES`, `EXTRA_CONDA_PACKAGES`, and `EXTRA_PIP_PACKAGES`), which, in conjunction with customized worker specs can be used to add dependencies at runtime. Some of them, like Coiled & Kubernetes allow for adding dependencies when building an image for our workers. Others, like YARN, use pre-allocated conda/virtual environment packing.

[WARNING]
====
It is very important to have the same version of Python and libraries installed on all of the workers and your client. Different versions of libraries can lead to outright failures or subtler data correctness issues.
====

=== Dask's Diagnostics User Interface

One of your first stops in understanding what your program is doing should be Dask's Diagnostics UI. The UI allows you to see what Dask is executing, number of worker threads/processes/computers, memory utilization information, and much more. If you are running Dask locally, you will likely find the UI at `http://localhost:8787`.

If you're using the Dask client to connect to a cluster the UI will be running on the scheduler node. You can get the link to the dashboard from `client.dashboard_link`.

[TIP]
====
For remote notebook users, the hostname of the scheduler node may not be reachable directly from your computer. One option is to use the jupyter proxy, for example I go to `https://jupyter.pigscanfly.ca/user/holdenk/proxy/dask-jovyan-4c81d51e-3.jhub:8787/status` to access the endpoint `dask-jovyan-4c81d51e-3.jhub:8787/status`.
====

The Dask UI during the running of the examples of this chapter is shown in <<fig_dask_ui>>.

[[fig_dask_ui]]
.The Dask UI
image::./figures/ch3-how-dask-works/DaskUI.png[]

The UI allows you to see what Dask is doing, what's being stored on the workers, and explore the execution graph. I'll revisit the execution graph more in the <<graph_visualize>>.

=== Serialization and Pickling

Distributed and parallel systems depend on serialization, sometimes called pickling in Python, to share data and functions/code between processes. Dask uses a mixture of serialization techniques to match the use case, and provides hooks to extend by class when the defaults don't meet your needs.

Cloudpickle serializes the functions and the generic Python types in Dask. Most Python code doesn't depend on serializing functions, but cluster computing often does. Cloudpickle is a project designed for cluster computing and is able to serialize & de-serialize more functions than Python's built-in pickle.

[WARNING]
====
Dask has its own ability to extend serialization, but the registry methods are not automatically sent to the workers, and it's not always used.footnote:[See dask-distributed github issues 5561 and 2953]
====

Dask has built-in special handling for numpy arrays, sparse, and cupy. These serializations tend to be more space-efficient than the default serializers. When you make a class that contains one of these types and does not require any special initialization, you should call `register_generic(YourClass)` from `dask.distributed.protocol` to take advantage of Dask's special handling.

If you have a class that is not serializable, like in <<fail_to_ser>>, you can wrap it to add serialization functions to it as shown in <<custom_ser_with_pickle>>.

[[fail_to_ser]]
.Dask Fails to Serialize
====
[source, python]
----
include::./examples/dask/Dask-Ch3-Concepts.py[tags=fail_to_ser]
----
====

[[custom_ser_with_pickle]]
.Custom Serialization
====
[source, python]
----
include::./examples/dask/Dask-Ch3-Concepts.py[tags=custom_serializer_not_own_class]
----
====

If you control the original class, you can also directly add the getstate / setstate methods instead of wrapping it as above.

[NOTE]
====
Dask automatically attempts to compress serialized data which generally improves performance. You can disable this by setting `distributed.comm.compression` to None.
====

[[basic_partitioning]]
=== Partitioning / Chunking Collections

Partitioning gives you the ability to control the number of tasks used to process your data. If you have billions of rows, using one task for each row would spend more time scheduling the tasks than the work itself.

Dask uses slightly different terminology for partitioning in each of its collections. Understanding partitioning is key to being able to make the most efficient use of Dask. In Dask Partitioning impacts how data is located on your cluster, and the right partitioning for your problem can make order of magnitude level improvements. Partitioning has a few different aspects, like size of each partition, number of partitions, as well as optional properties like partition key, and sorted v.s. unsorted.

The number and size of partitions are closely related and impact the maximum parallelism. Partitions that are too small, or too many partitions, means that Dask will spend more time scheduling the tasks than running them. A general "sweet spot" for partition size is around 100mb~1Gb, but if your computation per element is very expensive smaller partition sizes can perform better.

Ideally your partitions should be similar in size to avoid stragglers. When you've got partitions of different sizes it is called "skewed." There are many different sources of skew, ranging from input file sizes to key skewfootnote:[When keyed]. In addition to equal size you also want to make sure that you have enough partitions to take advantage of all the CPU cores in your cluster. On the flipside, having too many partitions causes an excessive number of tasks or excessive network traffic. When your data gets too skewed, you will need to re-partition the data.

[TIP]
====
The Dask UI is a great place to see if you might have stragglers.
====

Dask Array's partitions are called "chunks" representing the number of elements. Dask does not always know the size of each chunk, although it always knows the number of chunks, but when you apply a filter or load data Dask is unaware of the size of each chunk. Indexing or slicing a Dask array requires that Dask knows the size of each chunk so Dask can find the chunk(s) with the desired elements. Depending on how your Dask array was created, Dask may or may not know the size of each chunk. I'll talk about this more in the Dask collection chapter. If you want to index into an array where Dask does not know the chunk sizes you will need to first call `compute_chunk_sizes()` on the array.When creating a Dask array from a local collection, you can specify the target chunk size as shown in <<custom_array_chunk_size>>.

[[custom_array_chunk_size]]
.Custom Array Chunk Size
====
[source, python]
----
include::./examples/dask/Dask-Ch3-Concepts.py[tags=make_chunked_array]
----
====

Partitions/chunking doesn't have to be static, and the `rechunk` function allows you to change the chunk size of a Dask array.

Dask Bag's partitions are called partitions. Unlike with Dask Arrays, since Dask Bag's do not support indexing, Dask does not track the number of elements in each partition. When you use scatter, Dask will try to distribute the data as well as possible, but subsequent iterations can change the number of elements inside of each partition. Similar to Dask Arrays, when creating from a local collection you can specify the number of partitions of a bag, except the parameter is called `npartitions` instead of `chunks`.

You can change the number of partitions in a Bag by calling `repartition` with either `npartitions` (for a fixed number of partitions) or `partition_size` for a target size of each partition. Specifying `partition_size` is more expensive since Dask needs to do some extra computation to determine what the matching number of partitions would be.

You can think of data as keyed when there is an index, or the data can be looked up by a value â€“ like in a hashtable. While Bag implements "keyed" operations like groupBy where values with the same key are combined, it's partitioning does not have any idea of key and instead keyed operations always operate on all partitions.

Dataframes have the most options for partitioning. Dataframes can have partitions of different sizes, as well as known or unknown partitioning. With unknown partitioning, the data is distributed but Dask is unable to determine which partition holds a particular key. Unknown partitioning happens often as any operation which could change the value of a key results in unknown partitioning. The `known_divisions` property on a Dataframe allows you to see if Dask knows the partitioning, and the `index` property shows the splits used and the column.

If a Dataframe has the right partitioning, operations like groupBy which would normally involve a lot of inter-node communication can be executed with less communication. Accessing rows by id requires that the DataFrame is partitioned on that key. If you want to change the column that your DataFrame is partitioned on you can call `set_index` to change the index. Setting index, like all of the repartitioning operations, involves copying the data between workers, known as a shuffle.

[TIP]
====
The "right" partitioner for a dataset depends on not only the data but also your operations.
====

==== Shuffles

Shuffling refers to transferring data between different workers to repartition the data. Shuffling can be the result of an explicit operation, like calling repartition, or implicit like grouping data together by key or performing an aggregation. Shuffles tend to be relatively expensive, so it's useful to minimize how often they are needed and also reduce the amount of data they move.

The most straightforward case to understand shuffles are those when you explicitly ask Dask to repartition your data. In those cases you generally see many-to-many worker communication with the majority of the data needing to be moved over the network. This is naturally more expensive than situations where data is able to be processed locally as the network is much slower than RAM.

Another important way that shuffles happen is implicitly through a reduction/aggregation. In these cases, if parts of the reduction or aggregation can be applied prior to moving the data around, Dask is able to transfer less data over the network, making for a faster shuffle.

[TIP]
====
Sometimes you'll see things referred to as "map-side" and "reduce-side", this just really means before and after the shuffle.
====

We'll explore more about how to minimize the impact of shuffles in the next two chapters where we introduce aggregations.

==== Partitions During Load

So far you've seen how to control partitions when creating from a local collection as well as how-to change the partitioning of an existing distributed collection. Partitioning during the creation of a collection from delayed tasks is generally speaking 1:1, with each delayed task being its own partition. When loading data from files partitioning becomes a bit more complicated, like file layout and compression. Generally speaking, it is good practice to look at the partitioning of data you have loaded by calling `npartitions` for bags, `chunks` for arrays, or `index` for dataframes.

=== Tasks, Graphs, and Lazy Evaluation

Tasks are the building blocks that dask uses to implement dask.delayed, futures, and operations on dask's collections. Each task represents a small piece of computation that Dask can not break down any further. Tasks are often fine-grained, and when computing a result Dask will try and combine multiple tasks together into a single execution.

==== Lazy Evaluation

Most of Dask is lazily evaluated, with the exception of Dask futures. Lazy evaluation shifts the responsibility for combining computations together from you to the scheduler. This means that Dask will, when it makes sense, combine multiple function calls. Not only that, but sometimes if just parts of a structure are needed, Dask is able to optimize by evaluating only the relevant parts (like `head` or `tail` calls). Implementing lazy evaluation requires Dask to construct a task graph. This task graph is also re-used for fault-tolerance.

Unlike the rest of Dask, futures are eagerly evaluated, which limits the optimizations available when chaining them together as the scheduler has a less complete view of the world when it starts executing the first future. Futures still create task graphs, and you can verify this by visualizing them in the next section.

[[graph_visualize]]
==== Visualize

Visualizing the task graph is an excellent tool for you to use while learning about task graphs and debugging in the future. The visualize function is defined both in the dask library as well as on all Dask objects. Instead of calling `.visualize` separately on multiple objects, you should call `dask.visualize` with the list of objects you are planning to compute to see how Dask combines the task graph.

You should try this out now by visualizing everyone's favourite wordcount example from the previous chapter. When you call `dask.visualize` on `words_bag.frequencies()` you should get a result that looks something like <<vis_ex>>.

[[vis_ex]]
.Visualized wordcount task graph
image::./figures/ch3-how-dask-works/wc.pdf[]

[TIP]
====
The Dask UI also shows visualized representations of the task graph, without needing to modify your code.
====

==== Intermediate Task Results

Intermediate task results are generally removed as soon as the dependent task has started to execute. This can be less than optimal when we need to perform multiple computations on the same piece of data. One solution to this is combining all of our execution together into one call to `dask.compute`, so that Dask is able to keep the data around as needed. This breaks down in both the interactive case where we don't know in advance what our computation is going to be and iterative cases. In those cases, some form of caching or persistence can be beneficial. You will learn about how to apply caching later in this chapter.

==== Task Sizing

Dask uses a centralized scheduler, which is a common technique for many systems. It does mean however that while the general task scheduling overhead is 1ms, as the number of tasks in a system increases the scheduler can become a bottleneck, and the overhead can grow. Counter-intuitively this means as our system grows we may benefit from larger coarser-grained tasks.

==== Too large Task Graphs

Sometimes the task graph itself can become too much for Dask to handle. This can show up as an out-of-memory exception on the client or scheduler or more commonly jobs that slow down with iterations. Most frequently this occurs with recursive algorithms. One common example of a situation where the graph can become too expensive to keep is distributed alternating least squares.

The first step when encountering a situation with a too-large task graph is to see if you can reduce the parallelism by using larger chunks of work or switching the algorithm. For example, if we think of fibonacci numbers computed with recursion, a better option would be using a dynamic programming or memoized solution instead of trying to distribute the computation task with Dask.

If you have an iterative algorithm, and there isn't a better way to accomplish what you want, you can help Dask out by periodically writing out the intermediate work and reloading it.footnote:[You could also collect and scatter if the dataset is small enough.] By doing this Dask does not have to keep track of all of the steps involved in creating the data, but instead just needs to remember where the data is. The next two chapters will look at how to write and load data efficiently for these and other purposes.

[TIP]
====
In Spark the equivalent idea is expressed as "checkpointing."
====

==== Combining Computation

To take the most advantage of Dask's graph optimization, it's important to submit your work in larger batches. First off, when you're blocking on the result with `dask.compute`, small batches can limit the parallelism. If you have a shared parent, say two results on the same data, submitting the computations together allows Dask to share the computation of the underlying data. You can verify if Dask is able to share a common node by calling visualize on your list of tasks, e.g. if you take example <<shared_data>> and visualize both the tasks together you'll see the shared node in <<shared_compute_node>>.

Sometimes you can't submit the computation together, but you still know that you want to re-use some data. In those cases you should explore persistence.

==== Persist, Caching & Memoization

Persistence allows you to keep specified Dask collections in memory on the cluster. To persist a collection for future re-use just call `dask.persist` on the collection. If you choose persistence, you will be responsible for telling Dask when you are done with a distributed collection. Unlike Spark, Dask does not have an east `unpersist` equivalent, instead, you need to release the underlying futures for each partition as shown in <<manual_persist>>.

[[manual_persist]]
.Manual persistence and memory management with Dask
====
[source, python]
----
include::./examples/dask/Dask-Ch3-Concepts.py[tags=manual_persist]
----
====

[WARNING]
====
A common mistake is to persist and cache things that are not used more than once, or are inexpensive to compute.
====

Dask's local mode has a best-effort caching system based on cachey. Since this only works in local mode I won't go into the details, but if you are running in local mode you can take a look at https://docs.dask.org/en/stable/caching.html[the local cache documentation].

[WARNING]
====
Dask does not raise an error when you attempt to use Dask caching in a distributed fashion, it just won't work. So when migrating code from local to distributed make sure to check for usage of Dask's local caching.
====

=== Fault Tolerance

Dask's task graph is used to provide it's fault tolerance, as in Spark. Fault Tolerance becomes increasingly important the more computers you use. When you are using Dask on a single computer the concept of fault tolerance is less important since if your one computer fails there is nothing to recover. However, when you have 100s of machines, the odds of any one of those machines failing goes up. There are many different kinds of failures in a distributed system, but thankfully many of them can be handled in the same way.

Dask automatically retries tasks where the scheduler loses connection to the worker. This retry is accomplished by using the same graph of computation Dask uses for lazy evaluation.

[WARNING]
====
The dask client is *not* fault-tolerant to network issues connecting to the scheduler. One mitigation technique you can use is running your client in the same network as the scheduler.
====

Machine failure is a fact of life with distributed systems. When a worker fails, Dask will treat it in the same way as a network failure, retrying any necessary tasks. However, Dask can not recover from failures of the scheduler of your client code.footnote:[This is common for most systems like this. Spark does have limited ability to recover from head node failure, but has many restrictions and is not frequently used.] This makes it important that when you are running in a shared environment it is important to run your client and scheduler nodes at a high priority to avoid pre-emption.

Dask automatically retries software failures that exit or crash the worker. Much like machine failure, from Dask's point of view a worker exiting and a network failure look the same.

IOErrors and OSErrors exceptions are the only two classes of exceptions Dask will retry. If your worker process raises one of these errors, they are pickled and transferred over to the scheduler. Dask's scheduler then retries the task. If your code encounters an IOError which should not be retried (e.g. say a webpage which doesn't exist), you'll need to wrap it in another exception to keep Dask from retrying it.

Since Dask retries failed computation, it's important to be careful with side-effects or changing values. For example, if you have a Dask Bag of transactions and were to update a database as part of a `map`, Dask might re-execute some of the operations on that bag multiple times resulting in the update to the database happening more than once. If we think of a withdrawal from an ATM, we can see how this would result in some unhappy customers, and incorrect data. Instead if you need to mutate small bits of data you can bring it back to a local collection.

If your program encounters any other exceptions, Dask will return the exception to your main thread.footnote:[For those migrating from Spark, this retry behaviour is different. Spark will retry most exceptions, whereas Dask will only retry errors resulting in a worker exiting, or IOError or OSError.]

=== Conclusion

After this chapter you should have a good grasp of how Dask is able to scale your Python code. You should now understand the basics of partitioning, why this matters, task sizing, and Dask approach to fault tolerance. This will hopefully set you up well for deciding when to apply Dask and the next few chapters where we do a deeper dive on Dask's collection libraries. In the next chapter we'll focus on Dask's DataFrames as they are the most full-featured of Dask's distributed collections.
